# Multi-Agent System Benchmarks

This repository provides an overview of major multi-agent system benchmarks, adapted from the Galileo AI blog post: [Benchmarks and Use Cases for Multi-Agent AI](https://galileo.ai/blog/benchmarks-multi-agent-ai).

## Overview

Multi-agent AI systems leverage collaboration and competition among multiple agents to solve complex problems. Evaluating these systems requires specialized benchmarks that can assess interaction, communication, and coordination.

This repository provides a summary and comparison of the following key benchmarks:

*   **MultiAgentBench**: For LLM-driven multi-agent evaluation with a focus on modularity and various coordination topologies.
*   **BattleAgentBench**: For evaluating cooperative and competitive capabilities of language models.
*   **SOTOPIA-π**: For assessing social intelligence in immersive social scenarios.
*   **MARL-EVAL**: For standardized evaluation of multi-agent reinforcement learning systems.
*   **AgentVerse**: For evaluating diverse interaction paradigms and agent architectures.
*   **SmartPlay**: For evaluating strategic reasoning and planning in game environments.
*   **Who&When**: For failure attribution in multi-agent systems.

For a detailed comparison and analysis of each benchmark, please see the following versions:

*   [English Version](Overview_of_Multi-Agent_System_Benchmarks.md)
*   [Chinese Version](多智能体系统基准评测概述.md)
